
# DQN, A2C, and PPO Algorithm Differences

These are three different reinforcement learning algorithms used for training cache optimization models:

## DQN (Deep Q-Network)
- **Type**: Value-based method that approximates the Q-function
- **Action Space**: Works only with discrete action spaces
- **Features**: 
  - Uses experience replay buffer to improve stability
  - Off-policy algorithm (can learn from data not generated by current policy)
  - Good for problems with clear state-action value relationships

## A2C (Advantage Actor-Critic)
- **Type**: Policy gradient method with actor-critic architecture
- **Action Space**: Works with both discrete and continuous actions
- **Features**:
  - Actor determines actions, critic evaluates them
  - On-policy algorithm (learns from current policy's experiences)
  - More stable than pure policy gradients
  - Often more sample efficient than DQN

## PPO (Proximal Policy Optimization)
- **Type**: Policy gradient method with trust region optimization
- **Action Space**: Works with both discrete and continuous actions  
- **Features**:
  - Uses clipped objective to prevent destructively large policy updates
  - On-policy algorithm
  - Generally more stable and sample efficient than A2C
  - Often best performance on complex tasks with good hyperparameters

In your cache optimization context, PPO might provide the most robust performance, but all three can work depending on your specific requirements.

# Project Setup Guide

This guide outlines how to set up and run the Cache RL Optimization project locally, using Docker, or Docker Compose.

## Project Overview

The project is a reinforcement learning system for database cache optimization with GPU support. It provides an API to train models to optimize database query caching strategies.

## Prerequisites

- Python 3.11+
- MariaDB/MySQL
- NVIDIA GPU with CUDA support (optional, for faster training)
- NVIDIA drivers and CUDA toolkit (if using GPU)

## Local Setup

1. **Clone the repository**

   ```bash
   git clone <repository-url>
   cd cache-optimization
   ```

2. **Create a virtual environment**

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Configure database**

   Create a local MariaDB database:

   ```bash
   mysql -u root -p
   ```

   ```sql
   CREATE DATABASE cache_db;
   CREATE USER 'cacheuser'@'localhost' IDENTIFIED BY 'cachepass';
   GRANT ALL PRIVILEGES ON cache_db.* TO 'cacheuser'@'localhost';
   FLUSH PRIVILEGES;
   EXIT;
   ```

5. **Set environment variables**

   ```bash
   # Linux/Mac
   export DB_URL="mysql+mysqlconnector://cacheuser:cachepass@localhost:3306/cache_db"
   export CACHE_SIZE=20
   export MAX_QUERIES=1000

   # Windows
   set DB_URL=mysql+mysqlconnector://cacheuser:cachepass@localhost:3306/cache_db
   set CACHE_SIZE=20
   set MAX_QUERIES=1000
   ```

6. **Run the application**

   ```bash
   python main.py --host 0.0.0.0 --port 8000
   ```

   Access the API at `http://localhost:8000`

## Docker Setup

1. **Build and run with Docker**

   ```bash
   # Build the image
   docker build -t cache-optimization .

   # Run the container
   docker run -p 8000:8000 \
     -e DB_URL="mysql+mysqlconnector://cacheuser:cachepass@host.docker.internal:3306/cache_db" \
     -e CACHE_SIZE=20 \
     -e MAX_QUERIES=1000 \
     --name cache-optimization \
     cache-optimization
   ```

2. **Run with GPU support**

   ```bash
   # Ensure nvidia-docker is installed
   docker run -p 8000:8000 \
     -e DB_URL="mysql+mysqlconnector://cacheuser:cachepass@host.docker.internal:3306/cache_db" \
     -e CACHE_SIZE=20 \
     -e MAX_QUERIES=1000 \
     --gpus all \
     --name cache-optimization \
     cache-optimization
   ```

## Docker Compose Setup

1. **Start the services**

   ```bash
   docker compose up -d
   ```

   This starts both the database and API service with proper networking.

2. **With GPU support**

   Ensure your Docker Compose version supports the GPU configuration in the `docker-compose.yml` file:

   ```bash
   # Check GPU visibility (should show your GPUs)
   docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

   # Start services with GPU support
   docker compose up -d
   ```

3. **Access logs**

   ```bash
   # Follow application logs
   docker compose logs -f cache-rl
   ```

4. **Stop services**

   ```bash
   docker compose down
   ```

## Verification

1. Check API is running: `http://localhost:8000/health`
2. Training models will be stored in the `models` directory
3. Evaluation results will be stored in `cache_eval_results` directory

## Troubleshooting

- **Database connection issues**: Check if the MariaDB container is running and the connection URL is correct
- **GPU not detected**: Verify NVIDIA drivers and Docker GPU support with `nvidia-smi`
- **Python dependency errors**: Ensure you're using Python 3.11 with the correct packages

For more details, check the logs at `logs/cache_rl_api.log`


# API Endpoint

## Health Check

**Endpoint**: `GET /health`

```bash
curl -X GET http://localhost:8000/health
```

**Response**:
```json
{
  "status": "ok",
  "gpu_available": true
}
```

## Start Training

**Endpoint**: `POST /train`

```bash
curl -X POST http://localhost:8000/train \
  -H "Content-Type: application/json" \
  -d '{
    "db_url": "mysql+mysqlconnector://cacheuser:cachepass@mariadb:3306/cache_db",
    "algorithm": "ppo",
    "cache_size": 20,
    "max_queries": 1000,
    "timesteps": 50000,
    "feature_columns": ["query_type", "table", "timestamp"],
    "optimized_for_cpu": false,
    "use_gpu": true,
    "batch_size": 64,
    "learning_rate": 0.001
  }'
```

**Response**:
```json
{
  "job_id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
  "status": "pending",
  "start_time": "2023-09-15T10:23:54.123456"
}
```

## Get Job Status

**Endpoint**: `GET /jobs/{job_id}`

```bash
curl -X GET http://localhost:8000/jobs/f47ac10b-58cc-4372-a567-0e02b2c3d479
```

**Response**:
```json
{
  "job_id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
  "status": "completed",
  "start_time": "2023-09-15T10:23:54.123456",
  "end_time": "2023-09-15T10:45:12.987654",
  "model_path": "/app/models/ppo_cache_model_20230915_104512.zip",
  "metrics": {
    "hit_rate": 0.78,
    "miss_rate": 0.22,
    "avg_retrieval_time": 0.0023,
    "visualization": "/app/cache_eval_results/performance_20230915_104512.png"
  }
}
```

## List All Jobs

**Endpoint**: `GET /jobs`

```bash
curl -X GET http://localhost:8000/jobs
```

**Response**:
```json
[
  {
    "job_id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
    "status": "completed",
    "start_time": "2023-09-15T10:23:54.123456",
    "end_time": "2023-09-15T10:45:12.987654",
    "model_path": "/app/models/ppo_cache_model_20230915_104512.zip",
    "metrics": {
      "hit_rate": 0.78,
      "miss_rate": 0.22,
      "avg_retrieval_time": 0.0023
    }
  },
  {
    "job_id": "a1b2c3d4-e5f6-4a5b-9c8d-0e1f2a3b4c5d",
    "status": "running",
    "start_time": "2023-09-15T11:05:30.123456",
    "end_time": null,
    "model_path": null,
    "metrics": null
  }
]
```

## Evaluate Model

**Endpoint**: `POST /evaluate/{job_id}`

```bash
curl -X POST "http://localhost:8000/evaluate/f47ac10b-58cc-4372-a567-0e02b2c3d479?steps=2000&use_gpu=true"
```

**Response**:
```json
{
  "hit_rate": 0.82,
  "miss_rate": 0.18,
  "avg_retrieval_time": 0.0019,
  "memory_usage": 156.7,
  "total_queries": 2000,
  "cache_hits": 1640,
  "cache_misses": 360,
  "visualization": "/app/cache_eval_results/eval_20230915_112230.png"
}
```

## Python Examples

```python
import requests
import json

# Base URL
base_url = "http://localhost:8000"

# Health Check
response = requests.get(f"{base_url}/health")
print(response.json())

# Start Training
training_data = {
    "db_url": "mysql+mysqlconnector://cacheuser:cachepass@mariadb:3306/cache_db",
    "algorithm": "dqn",
    "cache_size": 15,
    "max_queries": 800,
    "timesteps": 40000,
    "use_gpu": True
}
response = requests.post(f"{base_url}/train", json=training_data)
job_id = response.json()["job_id"]
print(f"Started job: {job_id}")

# Get Job Status
status_response = requests.get(f"{base_url}/jobs/{job_id}")
print(json.dumps(status_response.json(), indent=2))

# List All Jobs
all_jobs = requests.get(f"{base_url}/jobs")
print(json.dumps(all_jobs.json(), indent=2))

# Evaluate Model (after job completes)
eval_response = requests.post(f"{base_url}/evaluate/{job_id}?steps=1500")
print(json.dumps(eval_response.json(), indent=2))
```